%% template for IEICE Transactions
%% v1.8 [2011/12/16]
\documentclass[paper]{ieice}
%\documentclass[invited]{ieice}
%\documentclass[survey]{ieice}
%\documentclass[invitedsurvey]{ieice}
%\documentclass[review]{ieice}
%\documentclass[tutorial]{ieice}
%\documentclass[letter]{ieice}
%\documentclass[brief]{ieice}
\usepackage[dvips]{graphicx}
\usepackage{times}

\usepackage{amsmath}
\usepackage{times}
\usepackage[varg]{txfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{comment}
\setcounter{page}{1}
%\breakauthorline{}% breaks lines after the n-th author

\field{}
%\SpecialIssue{}
%\SpecialSection{}
%\theme{}
\title{Fast Voting by Pyramid Match for Visual Object Detection}
%\title[title for header]{title}
%\titlenote{}
\authorlist{% fill arguments of \authorentry, otherwise error will be caused.
 %\authorentry{}{}{}
 %\authorentry[wangzp@cvl.iis.u-tokyo.ac.jp]{Zhipeng Wang}{membership}{affiliate label}
 \authorentry{name}{membership}{affiliate label}[present affiliate label]
 \authorentry[e-mail address]{name}{membership}{affiliate label}
 \authorentry[e-mail address]{name}{membership}{affiliate label}[present affiliate label]
}
\affiliate[affiliate label]{The author is with the  }
%\paffiliate[present affiliate label]{Presently, the author is with the }

%\received{2011}{1}{1}
%\revised{2011}{1}{1}
%\finalreceived{2011}{1}{1}

%% <local definitions here>

%% </local definitions here>

\begin{document}
\maketitle
\begin{summary}
Visual object detection is still a difficult task. This paper proposes an detection method which is very efficient in both learning and detection phases. Pyramid match is employed by the method to accelerate both learning and detection. Compared with state-of-the-art detection methods on two challenging datasets, the proposed method gives competitive experimental results with much more efficient  learning and detection.
\end{summary}
\begin{keywords}
Object detection, Fast voting
\end{keywords}

\section{Introduction}

Bag-of-features~\citep{obof,bgf} schema can be considered as the watershed between traditional and modern detection methods. Instead of considering each target object as a collection of raw optical elements, i.e., pixels, the schema tries to consider each object as a set of semantic elements or so-called object parts which are usually some strong local image features. Then one visual object is said to be a target object if it possesses some certain local features of certain numbers, while does not contain other certain local features of certain numbers. While this is quite straightforward, the very precious information encoded in local features' relative positions is left over. Following some pioneering ideas~\citep{spmk,ac30}, this chapter proposes a detection method which combines spatial and visual information of local image features in a  way pursuing both efficiency and effectiveness.

 Besides, inferring object status in a bottom-up manner fails to capture global information of each target object from the beginning. And this is also why recently the detection results of Hough transform based methods need refinement by discriminative methods in order to be competitive. Still the way how to use spatial information of local features is very illuminate.

 Just as said in~\citep{ac27}, Hough transform based methods and sliding-window methods are the two sides of the same coin. The method proposed in this chapter calculates confidence of a target object class for each sub-window in an image. Instead of considering each object as a collection of visual patterns (appearance of local features), the method considers each object as a set of visual-spatial patterns. One object is considered as a set of points. Each point is a digital vector, with the last two dimensions the relative $x-$ and $y-$ coordinates to object center, and SIFT after principal component analysis as the remaining dimensions. The training procedure is about collecting all such visual-spatial points into a point set, which acts as a super template. During detection, each sub-image is considered as a point set, and it is matched against the super template. The confidence is then the match score.

 The key to this method is how to define a match score for two point sets. Here pyramid matching procedure is employed, not only for efficiency, but also for combining visual and spatial information from local features in an effective manner. The visual-spatial space is divided from fine to coarse. Under a certain dividing parameter, points from the two matching point sets are considered as match if they fall into the same grid, and they are excluded from the respective point sets. The procedure continues till one point set is empty. Then the numbers of matched pairs under each dividing method is counted, and a weighted sum of all these numbers are considered as the match score for two point set, which will be referred to as Pyramid Match Score, or PMS for short. The weights under all dividing methods are learned during training, and how to divide the visual-spatial space is of great importance.

Obviously, each object is considered as a whole during detection in this method.

The proposed method also has several appealing properties, which include but not limited to:
\begin{itemize}
\item {Feasibility of sequential/batch training, which will lead to easy deployment in distributed system.}
\item {Time complexity of detection not related to the size of training examples.}
\end{itemize}



This paper is organized as follows. Section \ref{rw5} reviews most related work. Section \ref{dt5} proposes the training and detecting procedure. Section \ref{exp5} gives experimental results. Section \ref{dis5} compares time complexities between the proposed method with other methods, discusses about the insights of the method, and explains why the proposed method is effective. Section \ref{conc5} concludes.






\section{Related Work}
\label{rw5}


Detection methods still mainly follow the sliding-window schema or share similar structures with methods based on Hough transform. While the focus of the later is to infer about object status by use each online feature as a query against a well-trained codebook. These methods fail to consider target objects as a whole at the beginning. The problem of sliding-window schema is that it often ignores positional information when also following bag of features~\citep{bgf}. In the method of~\citep{spmk}, positional information is considered in the kernel function. Here a kernel function is usually used in classifiers, which are usually support vector machines, as introduced in~\citep{kmts}. The assumption behind~\citep{spmk} is that two images are considered as similar if they possess similar object parts at similar relative positions. Despite of the good theory, its being embedded in support vector machines as kernel function limits the efficiency of this method.


The Bag-of-features~\citep{bgf} schema successfully improves detection performance, while still there is information which are not made use of in images. The positional information is not fully made use of, even of the method of~\citep{kmts}. While~\citep{ac3} provides a method to model the relationship between object parts, there are too many parameters to estimate in their model, which requires a large amount of training data for acceptable performance.

In the method proposed in~\citep{ac222}, each object is modeled as a graph, when matching each object with another, constrains are made not only between the two objects, but also between different features of the same object. The relationship between elements of the same object is important. However, the inefficiency of this method prevents it from directly being used for object detection, while its performance on matching the same object under different views is promising.

The method in~\citep{lbt1} instead of building some parametric or non-parametric model, directly maps the labels of similar images in the training images to the current image. In this manner, the descriptive capacity of model will not affect performance, and this in return makes the method robust. However, this kind of methods heavily rely on the manually marked labels in the training dataset, while such labels are very expensive in human power.
\begin{comment}
The method in~\citep{ac3} considers both appearance model of object parts, and the relative distance changes between object parts. While giving promising results, there are too many parameters in the model, and training is troublesome when limited training images are available.
\end{comment}
The successes of HOG~\citep{ij4} on pedestrians benefit from its capability to encode relative spatial and visual information from each divided cells. Still the flexibility is not enough, and this leads to deformable part model~\citep{ac30,dpm1}, and its enhanced versions~\citep{ac31,dpm2}. The model will be referred as DPM for short. It is currently employed by most state-of-the-art methods considering  appearance information of object parts together with the relative positional information between object parts. In methods following DPM, a root template is used to detect each object as a whole, and HOG feature is usually used. When a potential object is detected, all possible object parts are detected accordingly. Finally, the confidence of the object is given by the confidence of the root object, the sum of confidence of the object parts, and the cost to deploy the object parts within the root object. Latent SVM is employed in these methods for optimization, and it is capable of representing information in a more complex form. Some methods motivated by DPM try to improve DPM by providing better solution searching strategies~\citep{dpm3}.  Two recent methods~\citep{spm,spltm} try to find sparse basis of object parts to reduce the number of parameters that need estimating. For efficiency, the method in~\citep{408} replace the dot operator of DPM with start-of-the-art hashing method~\citep{lsh}. There also exists hierarchical extensions of DPM~\citep{hdpm}, and multi-view extension of DPM~\citep{mvdpm}.

Advantages of DPM include that it only need to encode the object parts in positive training examples, and that  some of its invariants can give promising results in real time. Still DPM heavily rely of the latent SVM, which is trained in an  expectation-maximization manner, and this stops it from adopting new training examples. While nowadays, training examples often come sequentially. A very flexible model, which will evolve with training examples is preferred. These evolutions include evolving of object part number, evolving of the appearance models of object parts, and evolving of the relative positions of object parts.


The pyramid match score method is most related to methods using~\citep{pmk} or~\citep{kmts} as kernel functions, methods employ Hough transforms, and the methods proposing efficient solution space searching techniques~\citep{bab}. The method is also related to efforts trying to encode images~\citep{spen} and methods combining sliding window with Hough transform~\citep{ac18}.



\section{Pyramid Match Score}

\label{dt5}
In this section, firstly the typical procedure of pyramid matching is reviewed, and how a match score between two point sets by using pyramid matching is defined. Then based on the defined matric, how from the training examples, a super template can be learnt and how the super template can be used for object detection in a test image is proposed. In the definition of the matching score, there are parameters very important, finally in this section, how these parameters are estimated is introduced in three sub-sections.

In the remaining content of this chapter, all $i$s, $j$s and $k$s are local symbols.
\subsection{Pyramid Matching}



The Pyramid Matching method is designed to find the best one-one match, as shown in Figure \ref{fig:p2}, between two point sets in a heuristic manner.

Given two point sets, ${S_1} = \{ {u_1},{u_2},...,{u_m}\}, u_i \in {R^d}
$
 and ${S_2} = \{ {v_1},{v_2},...,{v_n}\}, v_i \in {R^d}$
, there exists a best one-one matching ${\pi}^*$ that minimizes the sum of $L1$
-distances between matched pairs,

\[
{\pi ^*} = \arg \mathop {\min }\limits_\pi  \sum\limits_{{u_i} \in {S_1}} {||{u_i} - {v_{\pi (i)}}|{|_1}} \ .
\]
Here $m  \le n$, and $\pi$ maps each feature $u_i$ in $S_1$ to a unique feature ${{v_{\pi (i)}}}
$ in $S_2$. There is a 2D example in Figure \ref{fig:pms1}

The best matching exists, and can be found by simple brute-force enumeration. In special cases, the Hungarian algorithm~\citep{ha} is also applicable.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pms1.eps}
\caption[Best one-one match]{A best one-one match problem in 2D space. There are two points in the first point set, three in the second point set. The arrows show correspondence between the two point set.}
\label{fig:pms1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pms2.eps}
\caption[Pyramid matching procedure]{Pyramid matching procedure which takes the 2D one-one match problem in Figure \ref{fig:pms1} as an example. The pyramid matching method divides the 2D space from fine to coarse in the 2D space. Notice, the triangle points belong to point set one, and the circle points belong to point set two. In the left, each dimension is divided into 4, results in totally 16 grids, and 0 matched point pairs are found. In the middle, each dimension is divided into 2, results in totally 4 grids, and two pairs of points belonging to different point sets are found and excluded. In the right, since the matched points belonging to matched point pairs are excluded, then only one point from the second point set is left. So the number of pairs found under all dividing methods are, 0, 2, and 0. The pyramid match score is calculated as a weighted sum of these 0, 2, and 0. }
\label{fig:p2}
\end{figure}




Sub-optimal solution can be found by heuristic methods. A very intuitionistic method is to find matched pairs of nearest distance, exclude corresponding points from both point sets, and repeat until no matched pair can be found.

The Pyramid Matching method is straightforward. Divide the point space from fine to coarse, find pairs of points from different point sets in the same grid under the current dividing parameter, exclude the matched pairs, and continue this procedure until the smaller point set is empty. The Pyramid Matching method is very efficient, and its time complexity is bounded by $O(dmL)$~\citep{pmk}. Here $d$ is the number of dimensions in each point set, $m$ is size of the smaller point set, and $L$ is number of dividing methods. In the example of Figure \ref{fig:p2}, $L$ is 3.

In~\citep{pmk}, pyramid matching helps to define kernel functions for SVMs. The meaning of pyramid matching is that, it changes how the way to define similarity between two objects. Originally two objects are considered as similar if they both contain certain number of certain object parts, while the idea of one-one match will only favor the objects parts which have corresponding counterparts.

Let $\gamma  = \{ {{\bf{g}}_1},{{\bf{g}}_2},...,{{\bf{g}}}_L\}$ be an ordered set, which contains all the dividing methods from fine to coarse. Let $N(S_1,S_2;{\bf{g}}_i)$ be the numbers of matched pairs of points under dividing method ${\bf{g}}_i$. Then the pyramid match score between $S_1$ and $S_2$ on $\gamma$ is defined by,

\begin{equation}
{\rm P}(S_1,S_2;\gamma)=\frac{{\omega}_1 \times N(S_1,S_2; {\bf{g}}_1) + \sum\limits^L_{i=2}{{\omega}_i\times( N(S_1,S_2;{\bf{g}}_i)-N(S_1,S_2;{\bf{g}}_{i-1}) ) }}{m}
.
\label{eq:pms1}
\end{equation}

To exactly follow the procedure as shown in Figure \ref{fig:p2}, the definition in (\ref{eq:pms1}) is rewritten as,

\begin{equation}
{\rm P}(S_1,S_2;\gamma)=\frac{ \sum\limits^L_{i=1}{{\omega}_i\times N(S_1^{(i-1)},S_2^{(i-1)};{\bf{g}}_i) }}{m}
.
\label{eq:pms2}
\end{equation}

Here, $S_1^i$ and $S_2^i$ represent the point set after excluding the points which are found match after the $i$th round matching respectively from $S_1^{(i-1)}$ and $S_2^{(i-1)}$. Actually, $S_1^0=S_1$, and $S_2^0=S_2$.

The procedure is  as follows, 1) given the original $S_1$ and $S_2$, find the point pairs which fall into the same grid in the space defined by ${\bf g}_1$, 2) exclude the matched points respectively from $S_1$ and $S_2$ to give $S_1^1$ and $S_2^1$, and 3) continue until $i=L$ or one point set is empty.

Then how to construct the dividing methods in $\gamma$ and how to define the corresponding weight, ${\omega}_i$, for each ${\bf{g}}_i \in \gamma$ are left to be defined.
And these also belong to the factors which distinguish the proposed method from~\citep{pmk}.


\subsection{Training and Detection}

The Pyramid Match Score is a matric between two point sets. In~\citep{pmk}, image features are considered as points, while in the proposed method, each point encodes both appearance and location information of each local feature. Each visual-spatial point is $d-$dimensional, and, the first $(d-2)$ dimensions are SIFT after PCA, while the last 2 dimensions are relative $x-$ and $y-$ coordinates after considering scale and width-height ratio changes.

Let ${\bf{p}}$ be a visual-spatial point in the point set of an image, $I$, and  $F_{\bf{p}}$ be the image feature of ${\bf{p}}$, which is $(d-2)-$dimentional.
Let $x_{\bf{p}}$ and $y_{\bf{p}}$ be the $x-$ and $y-$ coordinates of ${\bf{p}}$.  Let $w_I$ and $h_I$ be the width and height of $I$. Then

\[
{\bf{p}}=[F_{\bf{p}}^1,F_{\bf{p}}^2,...,F_{\bf{p}}^{d-2},\frac {x_{\bf{p}}} {w_I} , \frac {y_{\bf{p}}} {h_I}]\ .
\]


Instead of following~\citep{pmk}, PMS does not server as kernel functions for SVMs. And, a procedure similar to Hough transform is employed. Each training image is considered as a point set.  From all the training images, the method generates a point set as a super template, $S_{\bf{T}}$, following Algorithm \ref{alg:tm}. This is just a procedure to collect all points from point sets generated from training images into one point set.




\begin{algorithm}[chapter]





    \begin{algorithmic}[1]


       \STATE $S_{\bf{T}} \leftarrow \emptyset$

        \FOR{$S_{I_{tr}} \in \{S_{I_{tr}}\}$}

     \STATE  $S_{\bf{T}} \leftarrow S_{\bf{T}} + S_{I_{tr}}$

        \ENDFOR


    \RETURN $S_{\bf{T}}$

    \end{algorithmic}
    \caption{Template Generation}
    \label{alg:tm}


\end{algorithm}

In Algorithm \ref{alg:tm}, each $S_{I_{tr}}$ in $\{S_{I_{tr}}\}$ is the point set generated from the corresponding training image $I_{tr}$, and the $+$ operator is defined on two sets.

Actually, $S_{\bf{T}}$ plays a role similar to a codebook as in methods based on Hough transform.



For detection, a most popular pipeline is employed, as in Algorithm \ref{alg:dt}. All possible hypotheses are generated, given by $\{\eta \}$. Each hypothesis, $\eta$ is a rectangle in the image where target objects will be detected, and

\[\eta=[x_{\eta},y_{\eta},w_{\eta},h_{\eta}].\] So each $\eta$ is defined by its starting $(x,y)$ coordinate, its width, and its height. To generate $\{\eta \}$, the sliding window schema is followed, and it works by enumerating all possible rectangles by considering sub-windows' positions and sizes. In Algorithm \ref{alg:dt}, ${\Omega }$ is the set of final detection results, ${\rm P}_{th}$ is a threshold to accept hypotheses as detections, $I_{te}$ is a test image, and $S_{\eta}$ is the point set generated by local features contained in ${\eta}$.


\begin{algorithm}[chapter]






    \begin{algorithmic}[1]


       \STATE ${\Omega }  \leftarrow \emptyset$, generate $\{\eta \}$ from $I_{te}$

        \FOR{$ \eta \in \{\eta \}$}

     \STATE  Calculate ${\rm P}(S_{\eta},S_{\bf{T}};\gamma)$

        \ENDFOR


    \STATE Sort $\{\eta \}$ by ${\rm P}(S_{\eta},S_{\bf{T}};\gamma)$ in descending order

    \WHILE { ${\rm P}(S_{ \eta_1},S_{\bf{T}};\gamma) >= {\rm P}_{th}$ }
     \STATE  ${\Omega }  \leftarrow  {\Omega } + \eta_1$

       \STATE  $\{ \eta \}  \leftarrow  \{ \eta \} - \eta_1$

       \FOR {$\eta \in \{\eta \}$}
       \FOR {$\eta^{'} \in {\Omega } $}
       \FOR {$\bf{p} \in S_{\eta} $}

       \IF {$({\bf{p}}^{(d-1)},{\bf{p}}^d)$ is inside $\eta^{'}$}

        \STATE  $S_{\eta} \leftarrow S_{\eta} - \bf{p} $

       \ENDIF
         \ENDFOR
       \ENDFOR
       \STATE  Calculate ${\rm P}(S_{\eta},S_{\bf{T}};\gamma)$
       \ENDFOR
        \STATE Sort $\{\eta \}$ by ${\rm P}(S_{\eta},S_{\bf{T}};\gamma)$ in descending order
    \ENDWHILE

    \RETURN ${\Omega } $


    \end{algorithmic}

    \caption{Detection Procedure}
    \label{alg:dt}

\end{algorithm}






\subsection{Dividing Visual-spatial Space}

What is very important in Algorithm \ref{alg:dt} is how to define the set of dividing methods, $\gamma$. In the method of~\citep{pmk}, ${\bf g}_i$ means dividing each dimension of the point space into $2^i$ intervals. However, the space in~\citep{pmk} is a pure feature space, while the space here is a visual-spatial space. And also, in~\citep{pmk}, the two point sets both belong to objects, while here one point set belongs to the super template.

The space-dividing method proposed here divides the dimensions of visual features and spatial coordinates at different grid sizes. Let
\[{\bf g} = g(i,j),i,j \in N.\]
Here $g(i,j)$ is a function which defines how to divide the visual-spatial space. And $i$ means each dimension belonging to visual channel is divided in to $2^i$ intervals, and $j$ means each dimension belonging to spatial channel is divided into $2^j$ intervals. Note, that for a point, $\bf p$, the first $(d-2)$ dimensions belong to visual channel, while the remaining 2 dimensions belong to spatial channel. For example, if $d=3$, then $g(2,3)$ will divide the whole space into $(2^i)^{(d-2)}\times (2^j)^2=256$ grids.

In Figure \ref{fig:p3}, an example is given by considering visual information as one dimension, and spatial information as the other dimension. Note, the total dimension of a point is actually $d$, while in the example it is 2.

About $\gamma$, not only its members, but also the order of its members is important. For (\ref{eq:pms1}) to work, a requirement must be fulfilled, that if $i<j$, ${\bf g}_i$ is finer than ${\bf g}_j$, which means if two points are decided as match under ${\bf g}_i$, they must be decided as match under ${\bf g}_j$.  This is,



\begin{equation}
i<j, G({\bf p}_{S_1};{\bf g}_i)=G({\bf p}_{S_2};{\bf g}_i)\mbox{     }\mbox{     }\Rightarrow \mbox{     }\mbox{     }G({\bf p}_{S_1};{\bf g}_j)=G({\bf p}_{S_2};{\bf g}_j)\ .
\label{eq:pms3}
\end{equation}

If ${\bf g}_i$ is finer than ${\bf g}_j$, it is also written as ${\bf g}_i>{\bf g}_j$.

In (\ref{eq:pms3}), $G({\bf p};{\bf g})$ is a function to map {\bf p} into a particular grid, given dividing method ${\bf g}$. And $G({\bf p};{\bf g})$ on the $k$th dimension is defined by,
\[
G^k({\bf p};{g(i,j)})=
 \left\{ \begin{array}{*{20}{c}}
    \lfloor { \frac{2^i \times ({\bf p}_{max}^k - {\bf p}^k) }{{\bf p}_{max}^k - {\bf p}_{min}^k} }  \rfloor    &\mbox{  if } k \le (d-2) \\
    \lfloor { \frac{2^j \times ({\bf p}_{max}^k - {\bf p}^k) }{{\bf p}_{max}^k - {\bf p}_{min}^k} }  \rfloor  &\mbox{otherwise}
\end{array} \right. \:.
\]

Here, ${\bf p}_{max}^k$ and ${\bf p}_{min}^k$ are the maximum and minimum values on the $k$th dimension, which are determined by training dataset.



There is no such constrain which requires $\gamma$ is descending ordered by the fineness level in (\ref{eq:pms2}). Thus, in the following paper, (\ref{eq:pms2}) will be used.
In fact, when (\ref{eq:pms3}) is satisfied, (\ref{eq:pms1}) and (\ref{eq:pms2}) are the same.

Though (\ref{eq:pms2}) can be used to calculate a pyramid match score for two point sets, given any set, $\gamma$, still the dividing methods and the order of the dividing methods will affect performance. For a largest fineness level, $l_{max},l_{max}\in N$, $\gamma$ is defined in Algorithm \ref{alg:order}.

\begin{algorithm}[chapter]





    \begin{algorithmic}[1]


       \STATE $\gamma \leftarrow \emptyset$, $r \leftarrow 2\times (l_{max}-1)$

        \WHILE {$r \ge 0$}
        \IF {$r \ge {l_{max}-1}$}
        \STATE $i \leftarrow l_{max}-1$
        \ELSE
        \STATE $i \leftarrow r$
        \ENDIF
        \STATE $j \leftarrow r-i$
        \WHILE {$i \le (l_{max}-1)$ \AND $i \ge 0$ \AND $j \le (l_{max}-1)$ \AND $j \ge 0$}
        \STATE $\gamma \leftarrow \gamma + g(i,j)$, $i \leftarrow i-1$, $j \leftarrow r-i$
        \ENDWHILE
        \STATE $r \leftarrow r-1$
        \ENDWHILE




    \RETURN $\gamma$

    \end{algorithmic}
    \caption{Generation of Dividing Method Set}
    \label{alg:order}


\end{algorithm}



The size of $\gamma$, $L=l_{max} \times l_{max}$. For two dividing methods ${\bf g}_i,i \in {1,2,...,L}$ and ${\bf g}_j,j \in {1,2,...,L}$, if ${\bf g}_i>{\bf g}_j$, then $i<j$, which means if one dividing method is finer than the other, it will appear earlier in the set of dividing methods. There are also dividing methods, of which the fineness level cannot be compared, i.e., $g(1,2)$ and $g(2,1)$ as shown in Figure \ref{fig:p3}.


\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pms3.eps}
\caption[Visual-spatial space dividing]{An example set of methods to divide the visual-spatial space. $x-$ and $y-$ coordinates represent visual and spatial information respectively. From left to right, the first line is $g(2,2)$, $g(1,2)$, and $g(0,2)$. The second line is $g(2,1)$, $g(1,1)$, and $g(0,1)$. And the third line is $g(2,0)$, $g(1,0)$, and $g(0,0)$. And $\gamma$ is defined as an ordered set of all the dividing methods with different parameters, i.e., $\gamma=\{g(2,2),g(1,2),g(2,1),g(0,2),g(1,1),g(2,0),g(0,1),g(1,0),g(0,0)\}$.}
\label{fig:p3}
\end{figure}



\subsection{Deciding Weights for Dividing Methods}
After how to divide the visual-spatial space is decided, the remaining task is, for each dividing method $\bf g$, defining a corresponding weight.
When talking about two points which are found in the same grid under ${\bf g}=g(i,j)$, there is an upper bound to their $L1$-distance, which is given by
\[D_{ub}=  {(d-2)\times \frac 1 {2^i} +2\times \frac 1 {2^j} } ,
\]
 if unit length is assumed for all $({\bf p}_{max}^k - {\bf p}_{min}^k ),k \in \{1,2,...,d\}$. Since the first $(d-2)$ dimensions of each grid under $g(i,j)$ possess length of $\frac 1 {2^i}$, while the last 2 dimensions possess length of  $\frac 1 {2^j}$.



Following~\citep{pmk}, for two point from different point sets, if they are in the same grid under $g(i,j)$, which means $G({\bf p}_{S_1};g(i,j))=G({\bf p}_{S_2};g(i,j))$, the visual difference between ${\bf p}_{S_1}$ and ${\bf p}_{S_2}$  is defined as $ \frac {(d-2)}{2^i}$, and the spatial difference is defined as, $ \frac 2 {2^j}$.
A weight, $\omega$, defined for a dividing method $g(i,j)$ shows the importance of two matched points, and measures how difficult it is to match under such dividing method.

\begin{equation}
\label{eq:pms4}
\omega_{g(i,j)}=\sqrt{ ((d-2)\times{2^i})\times(2 \times {2^j})  }.
\end{equation}

As is seen in (\ref{eq:pms4}), the finer one grid is in $g(i,j)$, the larger a weight will be assigned for it. The weight is the confidence that the point set belong to a target object based on a point has corresponding evidence from the super template under the current $\bf g$.

\subsection{Learning Weights for Dividing Methods}
Besides directly assigning weights to all the dividing methods in a deterministic way, as in (\ref{eq:pms4}), a framework for learning weights is here proposed.

Often Gaussian kernels are used to measure differences between two features or two positions in Hough transform based methods following~\citep{lb1}.
For two visual-spatial points found match in the same grid under dividing method, $g(i,j)$,
the visual difference  is  $ \frac {(d-2)}{2^i}$, and the spatial difference is  $ \frac 2 {2^j}$.
The total difference between two points found match in the same grid is modeled using a 2D Gaussian kernel, as

\begin{equation}
\label{eq:pms5}
\omega_{g(i,j)}=\frac 1 {2 \pi \sigma_1 \sigma_2 \sqrt{1-\rho^2} }\exp(- \frac 1 {1-\rho^2} ( \frac {(\frac {(d-2)}{2^i})^2} {{\sigma_1}^2}
+ \frac {(\frac {2}{2^j})^2} {{\sigma_2}^2}  - \frac {2 \rho \frac {(d-2)}{2^i}  \frac 2 {2^j}}{\sigma_1 \sigma_2} )).
\end{equation}

%, and here reflects the confidence the whole point set is a target object based on the current matched pair.

In (\ref{eq:pms5}), $\rho$ is the correlation between visual and spatial channel, while $\sigma_1$ and $\sigma_2$ are standard deviations for visual and spatial channel.

To make (\ref{eq:pms5}) clear, it is rewritten as,

\begin{equation}
\label{eq:pms6}
\begin{aligned}
\omega_{g(i,j)}=& t \exp(-a (\frac 1 {2^i})^2 - b (\frac 1 {2^j})^2 + c (\frac 1 {2^i}) (\frac 1 {2^j}))\\
                =& t \exp(-a (\frac 1 {2^i})^2) \exp(- b (\frac 1 {2^j})^2 )\exp ( c (\frac 1 {2^i}) (\frac 1 {2^j})).
\end{aligned}
\end{equation}

Where,
\[\begin{aligned}
&t= \frac 1 {2 \pi \sigma_1 \sigma_2 \sqrt{1-\rho^2} } , \\
&a= \frac {(d-2)^2  }{(1-\rho^2){\sigma_1}^2},\\
&b= \frac{2^2 }{(1-\rho^2){\sigma_2}^2 },\mbox{ and}\\
&c= \frac{2 (d-2) (2)  \rho  }{(1-\rho^2){\sigma_1}{\sigma_2} }.
\end{aligned}
\]

In (\ref{eq:pms6}), the larger, the visual difference, or the larger, the spatial difference, the smaller the corresponding weight. This is decided by the 2D Gaussian kernel, and also this is consistent and very similar with Hough transform based methods.

In Algorithm \ref{alg:tm}, the weights are not needed, and the super template, $S_{\bf T}$, can be generated firstly. Then the performance of Algorithm \ref{alg:dt} will rely on the set of dividing method, $\gamma$, and each corresponding weight, $\omega$.


In (\ref{eq:pms6}), to define the weight for $g(i,j)$, besides $i$ and $j$, still there are four parameters, $t$, $a$, $b$, and $c$, need to be given. Here, $t$ is just a factor, it won't affect the results of Algorithm \ref{alg:dt}, which are the final detection results.

Here, $a$, $b$, and $c$ have their meanings. When $a$ is larger, visual information will play a more important role, and when $b$ is larger, spatial information will play a more important role. What is more interesting is that, there is $c$, which will be responsible for modeling correlating visual-spatial information.

To estimate $a$, $b$, and $c$ for a particular $\gamma$, all positive training images, $\{I_p\}$, and negative training images, $\{I_n\}$, are used. For brevity, the pyramid match score against the super template, with defined $\gamma$, is rewritten according to (\ref{eq:pms2}),
\begin{equation}
\begin{aligned}
{\rm P}(S_{I},S_{\bf{T}};\gamma)&=\frac{ \sum\limits_{i,j}{{\omega}_{g(i,j)}\times N(S_I^{(i-1)},S_{\bf T}^{(i-1)}; g(i,j)) }}{m}\\
&=t \sum\limits_{i,j}{  \exp(-a (\frac 1 {2^i})^2 - b (\frac 1 {2^j})^2 + c (\frac 1 {2^i}) (\frac 1 {2^j}))  \times \frac{ N(S_I^{(i-1)},S_{\bf T}^{(i-1)}; g(i,j))} m }\\
\end{aligned}
\label{eq:pms7}
\end{equation}

In (\ref{eq:pms7}), after summing up along $i$, and $j$ at given $\gamma$ and $S_{\bf T}$, it will be a function which changes according to $I$, $a$, $b$, and $c$. It is rewritten as ${\rm PMS}(I;a,b,c)$ for  brevity.



The objective function is written as the gap between pyramid match scores of positive training images and negative training images under normalizing condition as,

\[\begin{aligned}
&\arg \max\limits_{a,b,c} \dfrac{ \dfrac {\sum {\rm PMS}(I_p;a,b,c)}  {|\{ I_p \}|} - \dfrac {\sum {\rm PMS}(I_n;a,b,c)}  {|\{ I_n \}|} }
{\dfrac
{{\sum {\rm PMS}(I_p;a,b,c)}+ {\sum {\rm PMS}(I_n;a,b,c) }}
{|\{ I_p \}|+|\{ I_n \}|}
}\\
&\begin{aligned}
    s.t.:\mbox{ }& a>0;\\
    &b>0\;.
\end{aligned}
\end{aligned}
\]

Note, about ${\rm PMS}(I_p;a,b,c)$, it is not ${\rm P}(S_{I_p},S_{\bf{T}};\gamma)$, but ${\rm P}(S_{I_p},S_{\bf{T}}-S_{I_p};\gamma)$.

After all positive and negative training examples are given, the objective function will only contain $a$, $b$, and $c$. Thus for such a parameter estimating problem, brute-force solutions will be feasible, especially that $\exp({\bf \cdot})$ can be easily expanded.

Till now, Algorithm \ref{alg:tm} gives how $S_{\bf T}$ can be generated,  Algorithm \ref{alg:order}  gives how $\gamma$ is defined, how $\omega$ is estimated is given in (\ref{eq:pms4}) and (\ref{eq:pms7}) respectively, so Algorithm \ref{alg:dt} can be used for detection.


\section{Experimental Results}
\label{exp5}
The key value of the method is the proposed matric between an object hypothesis and the super template trained from training examples. Accordingly, two experiments are carried on and the results are reported in this chapter.


\subsection{UIUC Cars}

UIUC cars~\citep{cds} can be considered as one of the most famous datasets, and it has been used as benchmarks in the area of detection.
There are 1,050 training images, of which 550 are positive, and 500 are negative. There are 200 target objects in 167 test images without significant scale changes. In the evaluation following, together with the 200 target objects, 669 negative rectangles from the test images are extracted from testing images for the evaluation.

Performance of the method upon different parameters will be evaluated on the UIUC cars, and compared with DPM~\citep{ac31}.
In the experiments, (\ref{eq:pms4}) is used. So there are only two parameters left, which will lead to difference in performance, which are, the largest fineness level, $l_{max}$ and the dimension of SIFT after PCA, $(d-2)$.



\begin{figure}[!htbp]
\centering

\includegraphics[scale=0.75]{test-0_good.jpg}
\includegraphics[scale=0.75]{test-10_good.jpg}
\includegraphics[scale=0.75]{test-14_good.jpg}
\includegraphics[scale=0.75]{test-16_good.jpg}
\includegraphics[scale=0.75]{test-20_good.jpg}
\includegraphics[scale=0.75]{test-21_good.jpg}
\includegraphics[scale=0.75]{test-22_good.jpg}
\includegraphics[scale=0.75]{test-24_good.jpg}
\includegraphics[scale=0.75]{test-29_good.jpg}
\includegraphics[scale=0.75]{test-2_good.jpg}
\includegraphics[scale=0.75]{test-31_good.jpg}
\includegraphics[scale=0.75]{test-3_good.jpg}
\includegraphics[scale=0.75]{test-5_good.jpg}


\caption[Detection results on UIUC cars]{Detection results on UIUC cars~\citep{cds}. Yellow color marks ground truths, while blue marks detections.}
\label{fig:c5r}
\end{figure}

In Figure \ref{fig:c5r}, are some detection results on UIUC cars.


\begin{figure}[!htbp]
\centering

\includegraphics[width=1\textwidth]{pdim.eps}


\caption[Result evaluation using different dimensions]{Result evaluation on UIUC cars with different $(d-2)$s.}
\label{fig:c52}
\end{figure}
\FloatBarrier

In Figure \ref{fig:c52}, are the results given by using different $(d-2)$s. Precision-recall curves are generated directly by calculating pyramid match score. Obviously, when appearance information takes 10 dimensions, the performance is the best. The result is consistent with~\citep{pmk} and~\citep{lwz}.


\begin{figure}[!htbp]
\centering

\includegraphics[width=1\textwidth]{plmax.eps}


\caption[Result evaluation using different largest fineness levels]{Result evaluation on UIUC cars with different $l_{max}$s.}
\label{fig:c53}
\end{figure}
\FloatBarrier

In Figure \ref{fig:c53}, detection performance by using different $l_{max}$s is given. Note, the best performance is given at $l_{max}=5$. The reasons are two-fold: 1) the weights are given by (\ref{eq:pms4}), and favor larger fineness levels, and 2) both training examples and testing examples are of size $100 \times 40$, and $2^5=32$ is the best dividing parameter for positional information.

\begin{figure}[!htbp]
\centering

\includegraphics[width=1\textwidth]{pvp.eps}


\caption[Result evaluation by using results from only visual or spatial information]{Result evaluation on UIUC cars by using visual, spatial, and visual $+$ spatial information.}
\label{fig:c54}
\end{figure}
\FloatBarrier

In Figure \ref{fig:c54}, are the evaluation of visual and spatial information. Actually spatial information alone is not able to distinguish positive and negative target objects at all. The performance of just using visual information is much worse than using visual-spatial information.

\begin{figure}[!htbp]
\centering

\includegraphics[width=1\textwidth]{pnp.eps}


\caption[Result comparison with method using dividing methods of \citep{pmk}]{Result evaluation on UIUC cars by using $\gamma$ which is generated using Algorithm \ref{alg:order} with $l_{max}=5$ and $\gamma=\{g(4,4),g(3,3),g(2,2),g(1,1),g(0,0)\}$.}
\label{fig:c55}
\end{figure}
\FloatBarrier

How the visual-spatial space is divided is also critical to the performance of the method, and in Figure \ref{fig:c55}, results are evaluated by using the proposed space-dividing methods and \citep{pmk}.


\begin{figure}[!htbp]
\centering

\includegraphics[width=1\textwidth]{dpm.eps}


\caption[Result comparison with deformable part model~\citep{ac31}]{Result evaluation on UIUC cars between PMS and DPM.}
\label{fig:c56}
\end{figure}
\FloatBarrier


In Figure \ref{fig:c56}, performance are evaluated between the proposed method, and a state-of-the-art method, DPM~\citep{ac31}. The proposed method performs no worse than DPM,
while the training time between the two models are 1 minute vs 16 hours. In this experiment, the training speed of the proposed method is 1,000 times faster. Also during the training of the proposed method, since (\ref{eq:pms4}) is used, only the positive training examples are used, while DPM uses both positive and negative training examples.

\begin{figure}[!htbp]
\centering

\includegraphics[width=1\textwidth]{hcom.eps}


\caption[Comparison between Pyramid Match Score and Hough transform]{Compare the proposed method with different-speed Hough transforms.}
\label{fig:c5i}
\end{figure}
\FloatBarrier

In Figure \ref{fig:c5i}, performance are evaluated among the proposed method and variants of Hough transform methods. By taking into consideration of the size of the codebook, the original Hough transform method is 1,000 (the indexes in Figure \ref{fig:c5i} mean the times of time consumptions of the variants) time-consuming than the proposed method in giving the confidence of one sub-window's being a target object. Naive $k$-means method is employed to accelerate the original Hough transform method during training by condensing the size of the codebook. The original Hough transform method performs better than the proposed method. When the time consumptions are the same, the proposed method performs much better. And the proposed method performs better than the variant which is 4 times time-consuming.

\begin{figure}[!htbp]
\centering

\includegraphics[width=1\textwidth]{ancp.eps}


\caption[Average numbers of matched pairs of positive and negative test examples]{Comparison of average matched numbers between positive and negative test examples under different visual-spatial space dividing parameters.}
\label{fig:c57}
\end{figure}
\FloatBarrier

In Figure \ref{fig:c57}, the average numbers of matched pairs of positive and negative test examples are shown. It is clear that under finer dividing methods, positive test examples possess larger average numbers of matched pairs, while under coarser dividing methods, negative test examples possess larger matched numbers of matched pairs.


\subsection{P-campus}

The method is also tested on a dataset of pedestrians~\citep{wang1}. For a fair comparison, both training images and test images are exactly same for method comparisons.
In Figure \ref{fig:pcam}, there are some detection results on the dataset.

In Figure \ref{fig:cmf}, there are the results of comparing the proposed method with ordinary Hough transform and common fate Hough transform~\citep{wang1}. It can be seen that, common fate Hough transform performs the best since the method employs motion information. The proposed method almost perform as well as the Hough transform on this dataset.



The method detects at a frame rate of 8 frames per second with multi-thread accelerations, while common fate Hough transform needs two minutes to deal with one frame in its old implementation. For a very fair comparison, multi-thread accelerations are disabled for this method, and both \citep{ac9} and \citep{wang1} are re-implemented and re-compiled under the same settings. Then all three methods run on the same computer, and the time consumptions are reported in Table \ref{c5tb:tb1}. Obviously, the proposed method consumes the shortest time. There are more than 5,800 codes in the codebook, and the proposed method is more than 200 times faster than a method based on Hough transform to deal with one candidate sub-image in theory. When dealing with all candidate sub-images in one frame, it is only about 5 times faster. This is because that calculations for one sub-image can be cached and can be used for the decisions on its neighboring sub-images. For example, if two sub-images share the same image feature on the frame, and if best matched codes are found for this image feature, these codes can be used for decisions of both the sub-images.

\begin{figure}
\centering
\includegraphics[width=0.47\textwidth,bb=0 0 720 576]{org36.jpg}
\includegraphics[width=0.47\textwidth,bb=0 0 720 576]{org71.jpg}\\
\includegraphics[width=0.47\textwidth,bb=0 0 720 576]{org261.jpg}
\includegraphics[width=0.47\textwidth,bb=0 0 720 576]{org376.jpg}
\caption{Detection results on P-campus dataset.}
\label{fig:pcam}
\end{figure}


\begin{figure}[!htbp]
\centering

\includegraphics[width=1\textwidth]{cmf.eps}


\caption{Precision-Recall curves for the method, Hough transform, and common fate Hough transform.}
\label{fig:cmf}
\end{figure}


\begin{table}[h]
\centering
\begin{tabular}{lccc}
     \hline
     \hline
                               & Total time (ms) & Average time (ms)   \\
    \hline
    Proposed method      &	363,678 & 4,603 \\
    Hough transform      & 	1,986,583 & 25,146 \\
    Common fate Hough transform       &	8,296,161 & 105,014\\
   \hline
\end{tabular}
\caption[Comparisons of time consumptions]{Time consumptions of the proposed method, \citep{ac9}, and \citep{wang1}. Total time means the running time on all the 79 testing frames, and average time means average running time per frame.}\label{c5tb:tb1}
\end{table}

The training time of the three methods are also compared. Both Hough transform and common fate Hough transform spend 483 ms for training, while pyramid match score spend 978 ms for training. All methods spend less than one second for training.

\begin{comment}
\subsection{VOC2007 Cars}

Detection methods just all need to be evaluated on the PASCAL (pattern analysis, statistical modeling, and computational learning) Visual Object Classes~\citep{voc}. And VOC2007 is favored by even current famous detection methods~\citep{408}, because the testing data are publicly available, and many detection results are already evaluated on the dataset. To evaluate a detection method, experiments can be carried on one or several classes on the dataset, and here experiments are carried on objects belonging to the car category.

What is different from the experiments on UIUC cars is that, $\omega$s are given by (\ref{eq:pms5}) instead of (\ref{eq:pms4}). Also image features are not given by SIFT detectors, but by given by average sampling along edges from~\citep{bdt}. Also by setting threshold by the confidence from~\citep{bdt} of whether one  pixel is an edge point, that there are about 50 keypoints in all hypothesis rectangles are guaranteed.

\begin{figure}[!htbp]
\includegraphics[width=1.7\textwidth,bb=0 0 961 720]{v07car.png}
\caption[Result comparison on VOC2007 cars]{Result comparison on VOC2007 cars ({\color{red}Note, results by the proposed method still need to be added}).}
\label{fig:c57}
\end{figure}
\FloatBarrier
\end{comment}
\section{Discussion}
\label{dis5}
\subsection{Time Complexity}
The time complexity of find a sub-optimal best matching between two point sets is $O(dml_{max}^2)$ when pyramid matching is employed. Here, $d$ is the dimension of each visual-spatial point, $m$ is the size of the point set of each object hypothesis, and $l_{max}$ is the largest fineness level, which is usually 5. The match complexity of~\citep{pmk} is $O(dmL), L=l_{max}$. However, this is just the time complexity of matching between two point sets. To give the confidence of a hypothesis, the time complexity of the proposed method is still $O(dml_{max}^2)$, while for~\citep{pmk}, it is $O(dml_{max}n_{sup})$, where $n_{sup}$ is the number of support vectors in the used SVM, which is usually larger than 5. So, considering about the time complexity to give confidence for a hypothesis, the proposed method is no worse than~\citep{pmk}. Also $n_{sup}$ is related to the training dataset, while $l_{max}$ is less related. The case of ~\citep{spmk} is almost the same with~\citep{pmk}.

In theory, the time complexity of methods based on Hough transform based methods should be $O(dm{n_{tr}}^{\frac 1 {1+\varepsilon}})$, if the $k$NN framework is employed as in Chapter \ref{chp4}. Here $n_{tr}$ is the size of the codebook, which in the case of UIUC cars is about $50\times 500=25,000$. And, $\varepsilon > 0$ is a factor which affects the quality of the $k$NN. And compared to PMS, this is 1,000 times time-consuming. However, methods based on Hough transform usually does not consider about object detection in the sliding window manner, which makes such methods unable to well deal with scale changes.

The time complexity of DPM should be an exponential function to the number of parts used. Tricks can be used to avoid such heavy calculation in exchange of performance.

Methods employing bag-of-features schema, and using linear SVM have the lowest time complexity, which is usually  $O(dmn_{sup})$, while such methods are currently no longer the dominant.

In summary, the proposed method is competitive in the aspect of time complexity, when considering about giving a reliable confidence to an object hypothesis.

\subsection{Usage of Visual and Spatial Information}
Since there are two main information channels of the local features, the methods which make full use of such information should be robust and effective.

Let's take Figure \ref{fig:p3} as reference. Bag-of-features schema consider similarity between objects in $g(2,0)$. Actually,~\citep{pmk} considers matched image features in $\{g(2,0),g(1,0),g(0,0)\}$, while~\citep{spmk} considers in a single fineness level of visual information, i.e., $\{g(1,2),g(1,1),g(1,0)\}$. Generally speaking, method based on Hough transform only consider about the features in larger fineness level of both visual and spatial information, i.e., $\{g(2,2),g(2,1),g(1,2)\}$. And methods based on Hough transform, when in a bottom-up manner, can not deploy the object parts in a way similar to DPM. For example, an object part will contribute to an object it does not belong to.

Together with DPM, PMS considers about visual and spatial information at all fineness levels, and is able to model the correlation between visual and spatial information. However, the way is different. DPM picks several object part at a certain fineness level of visual information, and makes assumptions of their corresponding fineness level of spatial information. For example, DPM will use the most top-left grid of $g(2,1)$, and use the most top-right grid of $g(1,2)$. This grid-picking procedure is achieved by structure SVMs. In the case of PMS, the philosophy is different. Instead of finding some very representative features at certain locations, all features are used, the noisy features are averaged out, since a larger number, i.e., 50, of features are used, compared to DPM, which usually performs good with 6 local features.

Another advantage of PMS is that the number of parameters which need estimating is very small. While, $d=10$ is an easy conclusion. Other parameters, $l_{max}$, $a$, $b$, and $c$ are all super parameters, which actually contain abundant information. What is attractive is that even using (\ref{eq:pms4}) to directly assign weights, the performance on UIUC cars is still promising. These results are consistent with~\citep{pmk}.

To summarize, the way how the proposed method combines visual and spatial information of local features is theoretically promising.

\section{Chapter Conclusion}
\label{conc5}
This chapter proposes a method to efficiently and effectively combine visual and spatial information of the local image features. Experiments show the method is comparable with state-of-the-art detection methods on benchmark datasets. What make the method different are: 1) pyramid matching between a codebook and an  object hypothesis, 2) the set of methods to dividing the visual-spatial space, and 3) the way to define or learn weights for corresponding dividing method.

The underlying principle of PMS, which is defined by the order of dividing methods in Algorithm \ref{alg:order}, is that use the template to explain every visual-spatial point of an object hypothesis at the best visual-spatial level.

To the best of the author's knowledge, this is the first attempt of pyramid matching between a codebook and an object hypothesis.

\bibliographystyle{ieicetr}% bib style
\bibliography{egbib}



%\profile{}{}
%\profile*{}{}% without picture of author's face

\end{document}
